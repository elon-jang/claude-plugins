---
author: ''
date: '2026-01-29'
embedding_id: OpenAI Multi-Agent W-5222ec84
tags:
- ai-agent
- openai
- hackathon
- claude-code
- multi-agent
title: OpenAI Multi-Agent Workflows 해커톤 참여 후기 (feat. OpenAI 엔지니어가 Codex를 사용하는 방법)
url: https://lnkd.in/gFUmWsxD
---

OpenAI Multi-Agent Workflows 해커톤 참여 후기 (feat. OpenAI 엔지니어가 Codex를 사용하는 방법)
오늘, OpenAI가 공식 후원하고 Coxwave가 주최한 멀티-에이전트 워크플로우 해커톤에 코르카 동료 하동훈님과 함께 참여했습니다.

자세한 내용은 블로그에 있습니다:[https://lnkd.in/gFUmWsxD](https://lnkd.in/gFUmWsxD)

---

최근 코르카는 AI Science에 많은 관심이 있었기 때문에 과학 연구 가속화를 돕는 에이전트를 만들어보고자 했고요. AI 시대 이후로는 해커톤에 처음 참여해보는 거라서 준비를 많이 했어야 했지만... 실제로는 다른 급한 일들 처리하다 보니 거의 준비할 시간이 없었습니다. 😂

간신히 주말 + 전날 밤에 동훈님과 함께 회의하면서 큰 그림을 싱크했고, 이런 식으로 메타 전략을 짰어요.

1. 50분마다 알림 맞춰놓고 회고를 하며 현재 상태와 배운 점을 공유하고, 방향 전환을 위한 기회로 삼자.
2. (가장 큰 명세서이자 AGENTS.md에 들어갈 + README가 될) 문서부터 시작하기. 이 시스템은 누구의 어떤 문제를 푸는가? 그 문제는 왜 가치있는가? 데모 시나리오는 어떻게 되나(입력은 무엇이고 출력은 무엇인가)? 아키텍처는?
3. 처음부터 살아있게 만들자. 가장 처음부터 (굉장히 얇은 상태로) 데모 시나리오가 가능한 상태로 만들고 여기에 하나씩 붙여나가자.
4. Eval이 매우 중요. 에이전트에게 넣을 데이터의 품질은 어떻게 평가하고 개선할 것인가? 데모 시나리오의 성공 기준은 무엇으로 잡을 것인가? 각 에이전트의 성공 기준을 무엇으로 잡을 것인가? 각 기준을 어떻게 자동으로 평가하는가? (dspy를 쓰면 어떨까?)

OpenAI의 Agents SDK를 준비하면서 처음 써봤는데 한국어 문서도 있고, 아주 다양한 부분에서 사려깊게 구현되어있어서 놀랐습니다.

# OpenAI 엔지니어가 Codex를 사용하는 방법

해커톤을 시작하기 전에 OpenAI의 솔루션 아키텍트인 Tyler Ryu께서 (다행히 한국어로) 유익한 발표를 해주셨어요. 발표자료가 슬라이드인 줄 알았는데 모든 게 Codex로 구현되어, 라이브 데모까지 가능한 웹사이트였다는 게 놀라웠습니다. 간단하게 요약해봅니다.

## 유용한 Codex 사용 패턴

타일러는 Codex를 주로 이렇게 쓰신다고 하더군요.

> codex --yolo --search --enable collab

1. --yolo: 권한 모두 허용해둔 것. 당연히 조심해서 써야 함.
2. --search: 기본적으로 codex는 웹서치가 막혀 있음. 이거 해야 열림.
3. --enable: 베타 버전 기능들은 이걸로 켜야 함
4. collab: 작업을 서브에이전트로 동작시키는 것. spawn 키워드가 프롬프트에 있으면 작동함. 1/20 새벽에 추가된 기능.

그리고 참가자 질문 중 '클코 대비 Codex가 뭐가 좋은가?' 가 재미있었는데요. 타일러는 어디까지나 개인 의견임을 강조하시며 이렇게 얘기하셨습니다.

- Codex가 실행 시간은 좀 더 길지만, 생산되는 코드 퀄리티가 조금 더 높은 느낌.
- 아주 거대한 저장소에서 잘 동작함. OpenAI에서 과학 연구도 다 Codex로 하고, 죄다 모노리포라서 클론받으면 몇십GB쯤 되는데 그래도 잘 됨.
- 오토 컴팩션 성능이 좋아서 컨텍스트 걱정을 최근에 한 적이 없음. 10번 컴팩션돼도 잘 동작하더라.
- 명령을 굉장히 잘 따름. 좋은 명령 내릴 수 있다면 추천.

저는 다른 것보다 오토 컴팩션의 성능에 놀랐습니다. 마침, 관대하게도 모든 참여자들에게 OpenAI API 크레딧 $100과 ChatGPT Pro 1년 플랜이 지급되었습니다. 덕분에 오늘은 작정하고 Codex를 extra-high effort로 써봤고, 실제로 컨텍스트 제약이 별로 느껴지지 않았어요.

## 멀티-에이전트 워크플로우 패턴

또한 타일러는 OpenAI가 다양한 고객사에서 만나는 패턴을 6개로 정리해주셨습니다.

1. Manager-Worker: 가장 일반적. Cursor에서 최근에 Codex 써서 웹브라우저 처음부터 만들었는데 그것도 이렇게 한 것. 오케스트레이터가 서브에이전트에게 작업 할당 후 전달받아서 병합
2. Role-Specialized Swarm: Manager-Worker와 유사하지만 병합하기보다는 각자 다른 역할을 함. 유저가 다면적 보고서를 다 보고 종합할 수 있기 때문에 리서치에 유용함
3. Debate / Adversarial: 서로 논의 + 역할연기 가능. 무한 논의 안 하게 종료 컨디션 필수(이건 다른 패턴도 유사). 법조계에서 유용하게 쓸 수 있음
4. P2P Group Chat: 브레인스토밍에 좋음. 여기도 완료기준 등 제약 걸어놔야 함. 단순한 난상토론이 되지 않게, 서로가 서로를 멘션할 수 있게 하면 좋음
5. Blackboard: 코드베이스 + Codex가 한 예시. 장기 실행 워크플로우. 스크래치패드랑 비슷함
6. Planner-Executer-Verifier: 성공 선언 후 명시적 검증. 마지막에 validation해서 계속 점검. 점검하는 에이전트는 reasoning effort를 높이는 게 유리함. Verifier 루프에서 시간이 오래 걸릴 수 있어서, 시간이 안 중요한 케이스에서 쓰기 좋음

비록 Top 5 수상이라는 결과는 얻지 못했지만 무척 가치있는 경험을 많이 얻었습니다. Codex의 컨텍스트 관리 능력과 구현 능력에 감탄했고, OpenAI Agents SDK는 개발자가 신경써야 할 것들을 대부분 간결하게 추상화해주었습니다. 이번에 받은 ChatGPT Pro Plan을 당분간 Claude Code와 함께 적극 활용해보려 합니다.

해커톤에서 느낀 점, 잘 된 것과 잘 안 된 것은 블로그에 자세히 써두었습니다. 요즘 해커톤이 어떻게 돌아가는지 궁금하신 분은 한번 읽어보시길!

---
## AI Notes
- **Summary**: 오늘, OpenAI가 공식 후원하고 Coxwave가 주최한 멀티-에이전트 워크플로우 해커톤에 코르카 동료 하동훈님과 함께 참여했습니다. 자세한 내용은 블로그에 있습니다:[https://lnkd.in/gFUmWsxD](https://lnkd.in/gFUmWsxD)
- **Topics**: general